#!/usr/bin/env python
"""
vis.py — Test‑set Reporting & Visualization Toolkit
=================================================

Purpose:
--------
This script evaluates a pre-trained Transformer model (from test.py) on the
test set and generates various publication-ready figures for analysis and
interpretation. It focuses on visualizing model performance, residuals,
feature importance (via attention pooling and Integrated Gradients), and
raw feature characteristics.

Prerequisites:
--------------
Requires the outputs generated by `test.py`:
- The trained model weights file (e.g., `transformer_predictor_state.pth`).
- The feature scaler file (e.g., `scaler.joblib`).
- The test data files (`static_embedding.csv`, `test.txt`) must be present
  in the specified data directory.

Key Features:
-------------
- Loads the test dataset and applies the same scaling used during training.
- Loads the pre-trained Transformer model state.
- Performs inference on the test set to get predictions.
- Calculates and reports key performance metrics: Spearman ρ, Pearson r, MSE, MAE.
- Generates and saves the following figures:

    1. `predictions_vs_targets.png`:
        - **What:** Scatter plot comparing the model's predicted scores (y-axis) against the true human-assigned scores (x-axis) for the test set. Includes an ideal prediction line (y=x) and the overall Spearman correlation (ρ) in the title. Helps assess prediction accuracy and linearity.
        - **How:** The pre-trained model generates predictions on the scaled test features. These predictions are plotted against the true target scores from the test set.

    2. `prediction_distribution.png`:
        - **What:** Overlaid histograms showing the distribution of the model's predicted scores and the distribution of the true target scores from the test set. Helps visualize if the model's output distribution matches the target distribution and identify potential biases.
        - **How:** Collects all model predictions and true scores from the test set. Generates and plots histograms for both sets of values on the same axes.

    3. `residuals_vs_targets.png`:
        - **What:** Scatter plot of prediction errors (residuals = predicted - true) against the true target scores. Helps diagnose if errors are consistent across the score range or show patterns (e.g., heteroscedasticity). Ideally, residuals should be randomly scattered around zero.
        - **How:** Calculates residuals by subtracting true scores from model predictions for each test sample. Plots these residuals against the corresponding true scores.

    4. `attention_heatmap_test01.png`:
        - **What:** Visualizes an approximation of the Transformer encoder's self-attention mechanism for the *first image* processed in the test set. It shows a heatmap where rows and columns represent feature dimensions. Cell (i, j) intensity indicates how much the model's representation of feature `i` was influenced by feature `j` *based on the final encoder output*. It approximates learned feature interactions.
        - **How:** The model's `_encode_with_attn` method calculates a similarity matrix (using dot product) between the final hidden states of the Transformer encoder for the first test batch. This matrix is softmax-normalized to resemble attention weights. The map for the first image (`A[0]`) is plotted. *Note: This is an approximation based on final outputs, not the direct multi-head attention weights.*

    5. `dim_importance_barplot.png`:
        - **What:** Bar plot showing the average importance assigned to each input feature dimension by the *attention pooling* layer across the entire test set. Higher bars indicate features that received greater weight (`alpha`) when aggregating the Transformer's sequence output for the final prediction.
        - **How:** The attention pooling weights (`alpha`) are extracted from the `attn_pool` layer for every test sample. These weights are averaged across all samples for each feature dimension, and the means are plotted.

    6. `ig_importance_barplot.png` (Optional):
        - **What:** Bar plot showing feature importance based on Integrated Gradients (IG). It quantifies how much each input feature contributed to the model's output score, averaged over a subset of the test set. Higher absolute IG values suggest greater feature influence.
        - **How:** Uses the `captum` library. The IG algorithm computes attributions for each feature by integrating gradients along a path from a baseline input to the actual input, run on the pre-trained model for a subset of test samples. The mean absolute attributions are plotted.

    7. `feature_correlation_heatmap.png`:
        - **What:** Heatmap of the Pearson correlation matrix calculated from the *scaled* input features of the test set. Visualizes the linear relationships between feature dimensions *after* standardization. Helps understand the data structure the model receives.
        - **How:** Calculates the Pearson correlation coefficient between all pairs of feature dimensions using the scaled test features (`X_test`). The resulting matrix is plotted as a heatmap.

Usage:
------
Run the script from the command line, providing paths to the necessary files:

```bash
python vis.py --data_dir /path/to/your/data \
              --weights /path/to/your/transformer_predictor_state.pth \
              --scaler /path/to/your/scaler.joblib \
              [--fig_dir ./Figures] \
              [--batch 128] \
              [--cpu] \
              [--skip_ig]
```

Arguments:
----------
- `--data_dir`: Path to the directory containing `static_embedding.csv` and `test.txt`.
- `--weights`: Path to the saved model state dictionary (`.pth` file).
- `--scaler`: Path to the saved scaler (`.joblib` file).
- `--fig_dir` (optional): Directory to save the output figures (default: `./Figures`).
- `--batch` (optional): Batch size for inference (default: 128).
- `--cpu` (optional): Force the use of CPU even if CUDA is available.
- `--skip_ig` (optional): Skip the potentially time-consuming Integrated Gradients calculation.

Outputs:
--------
- Console output with test set performance metrics (Spearman, Pearson, MSE, MAE).
- PNG image files saved in the specified `--fig_dir`.

Dependencies:
-------------
- PyTorch
- Pandas
- Scikit-learn
- NumPy
- Matplotlib
- Tqdm
- Joblib
- Captum (optional, for Integrated Gradients)
"""
import os, argparse, math, logging, sys
import numpy as np
import pandas as pd
import torch, torch.nn as nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from scipy.stats import spearmanr, pearsonr
import joblib
from tqdm import tqdm
import torch.nn.functional as F # Add F for softmax in AttnPool

# --- REMOVE Import Model Definition from test.py ---
# try:
#     from test import TransformerPredictor # Import the model class
#     from test import NUM_FEATURES, D_MODEL, N_HEAD, NUM_ENCODER_LAYERS, DIM_FEEDFORWARD, DROPOUT # Import constants
# except ImportError as e:
#     print(f"Error importing from test.py: {e}")
#     sys.exit(1)

# --- Re-add Model Definitions (copied *exactly* from test.py) ---

# Copied from test.py
class AttnPool(nn.Module):
    """Learnable Attention Pooling layer."""
    def __init__(self, d_model):
        super().__init__()
        # Simple linear layer to compute attention scores
        self.attention_weights = nn.Linear(d_model, 1)

    def forward(self, h):
        # h: (B, S, d_model) - Output from transformer encoder
        # Compute scores
        scores = self.attention_weights(h) # (B, S, 1)
        # Apply softmax over the sequence dimension (S)
        alpha = F.softmax(scores, dim=1) # (B, S, 1)
        # Weighted sum
        pooled = torch.sum(h * alpha, dim=1) # (B, d_model)
        # Return both pooled output and attention weights (alpha)
        # Squeeze alpha for easier handling: (B, S)
        return pooled, alpha.squeeze(-1)

# Copied from test.py
class TransformerPredictor(nn.Module):
    # Using parameters consistent with test.py's definition
    def __init__(self, num_features=66, d_model=256, nhead=16, num_encoder_layers=8, dim_feedforward=1024, dropout=0.01, return_attn=False): # Match test.py defaults/names
        super().__init__()
        self.num_features = num_features
        self.d_model = d_model
        self.return_attn = return_attn # Keep for consistency, though hook is main goal

        # 1. Token embedding
        self.token_embed = nn.Linear(1, d_model)

        # 3. Dimension position embedding
        self.dim_embed = nn.Embedding(num_features, d_model)
        self.register_buffer('dim_id', torch.arange(num_features))

        # 4. Self-attention encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True,
            norm_first=False # Match test.py
        )
        # Ensure average_attn_weights=False for hook compatibility
        encoder_layer.self_attn.average_attn_weights = False

        # Use the exact name as in test.py: 'transformer_encoder'
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_encoder_layers
        )

        # 5. Read-out head using AttnPool
        self.attn_pool = AttnPool(d_model)

        # Use the exact name as in test.py: 'prediction_head'
        self.prediction_head = nn.Sequential(
            nn.Linear(d_model, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    # Add the attention extraction helper method again
    def _encode_with_attn(self, h):
        # First, run the transformer encoder normally
        encoded_h = self.transformer_encoder(h)
        
        # Now calculate attention weights separately based on the encoded features
        # This is a simplified approach that doesn't extract the exact internal attention
        # weights but gives a close approximation
        
        # Create attention map representing feature relationships
        batch_size, seq_len, d_model = encoded_h.shape
        
        # Method 1: Simple dot-product between features
        # Normalize features for better attention visualization
        normalized_h = F.normalize(encoded_h, p=2, dim=2)  # Normalize along feature dimension
        
        # Calculate dot product attention between all positions
        attn_map = torch.bmm(normalized_h, normalized_h.transpose(1, 2))
        
        # Apply softmax to get proper attention weights
        attn_map = F.softmax(attn_map, dim=2)
        
        logging.info(f"Successfully calculated attention map with shape {attn_map.shape}")
        
        return encoded_h, attn_map

    # Update forward to use the correct attribute names and the hook
    def forward(self, x_raw, require_maps=False):
        x = x_raw.unsqueeze(-1)         # (B, S, 1), S=num_features
        x_tok = self.token_embed(x)     # (B, S, d_model)
        dim_ids = self.dim_id.to(x_tok.device)
        h_in = x_tok + self.dim_embed(dim_ids) # (B, S, d_model)

        if require_maps:
            h, A = self._encode_with_attn(h_in) # h:(B,S,d_model), A:(B,S,S)
        else:
            h = self.transformer_encoder(h_in)  # Use correct attribute name
            A = None

        pooled, alpha = self.attn_pool(h)
        pred = self.prediction_head(pooled)  # Use correct attribute name

        if require_maps:
            if A is None:
                 logging.error("Attention map requested but not generated.")
                 A = torch.zeros(h.size(0), self.num_features, self.num_features, device=h.device)
            return pred, alpha, A
        else:
            return pred

# --------------------------  DATA HELPERS  ------------------------------- #
class HumanRatingDataset(torch.utils.data.Dataset):
    def __init__(self, features, targets):
        self.X = torch.tensor(features, dtype=torch.float32)
        self.y = torch.tensor(targets,  dtype=torch.float32).unsqueeze(-1)
    def __len__(self): return len(self.X)
    def __getitem__(self, idx): return self.X[idx], self.y[idx]

def load_test(data_dir, scaler_path):
    embed_csv  = os.path.join(data_dir, "static_embedding.csv")
    test_txt   = os.path.join(data_dir, "test.txt")
    # read id & target
    ids, targets = [], []
    with open(test_txt) as f:
        for ln in f:
            p = ln.strip().split()
            if len(p)==2:
                ids.append(p[0]); targets.append(float(p[1]))
    df = pd.read_csv(embed_csv, dtype={'image': str}).set_index('image')
    X, y = [], []
    for img, t in zip(ids, targets):
        if img in df.index:
            X.append(df.loc[img, df.columns[:66]].values)
            y.append(t)
    X = np.asarray(X, dtype=np.float32)
    y = np.asarray(y, dtype=np.float32)
    scaler = joblib.load(scaler_path)
    X = scaler.transform(X)
    return X, y

# --------------------------  PLOTTING UTILS  ----------------------------- #
plt.rcParams.update({'figure.dpi': 140})

def savefig(fig, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    fig.tight_layout(); fig.savefig(path); plt.close(fig)

# ------------------------------------------------------------------------ #

def main(args):
    logging.basicConfig(level=logging.DEBUG, # Consider logging.DEBUG for hook details
                        format='%(asctime)s - %(levelname)s - %(message)s',
                        handlers=[logging.StreamHandler(sys.stdout)])
    device = torch.device('cuda' if torch.cuda.is_available() and not args.cpu else 'cpu')
    logging.info(f"Using device: {device}")

    # 1) Load data
    X_test, y_test = load_test(args.data_dir, args.scaler)
    logging.info(f"Loaded test set: {len(y_test)} samples")
    test_loader = DataLoader(HumanRatingDataset(X_test, y_test), batch_size=args.batch, shuffle=False)

    # 2) Load model - Use the locally defined TransformerPredictor
    # Use hardcoded hyperparameters matching the original/test.py defaults
    # These should match the constants in test.py (NUM_FEATURES=66, D_MODEL=256, N_HEAD=16, etc.)
    NUM_FEATURES=66
    D_MODEL=256
    N_HEAD=16
    NUM_ENCODER_LAYERS=8
    DIM_FEEDFORWARD=1024 # 4 * D_MODEL
    DROPOUT=0.01

    model = TransformerPredictor(
        num_features=NUM_FEATURES,
        d_model=D_MODEL,
        nhead=N_HEAD,
        num_encoder_layers=NUM_ENCODER_LAYERS, # Use correct parameter name
        dim_feedforward=DIM_FEEDFORWARD,       # Use correct parameter name
        dropout=DROPOUT
    ).to(device)
    logging.info("Using locally defined TransformerPredictor class.") # Add log confirmation


    # Load state dict - should work as the class definition now matches test.py's structure
    try:
        model.load_state_dict(torch.load(args.weights, map_location=device, weights_only=True))
        logging.info(f"Successfully loaded model weights from {args.weights}")
    except FileNotFoundError:
        logging.error(f"Weight file not found: {args.weights}")
        sys.exit(1)
    except RuntimeError as e:
         logging.error(f"RuntimeError loading state_dict: {e}")
         logging.error("This might indicate a mismatch in model architecture hyperparameters (layers, dimensions) between training and visualization.")
         sys.exit(1)
    except Exception as e:
        logging.error(f"An unexpected error occurred loading the model: {e}", exc_info=True)
        sys.exit(1)

    model.eval()

    # holders
    preds, trues = [], []
    alpha_sum = torch.zeros(NUM_FEATURES, device=device) # Use constant NUM_FEATURES
    first_batch_A = None # Variable to store attention map of the first batch

    with torch.no_grad():
        for i, (xb, yb) in enumerate(tqdm(test_loader, desc="Infer", unit="b")):
            xb = xb.to(device)
            # Call the forward method with require_maps=True
            y_hat, alpha, A = model(xb, require_maps=True) # A shape: (B, S, S)
            preds.append(y_hat.squeeze().cpu())
            trues.append(yb.squeeze())
            alpha_sum += alpha.sum(0)

            # Store the attention map from the first batch only
            if i == 0:
                first_batch_A = A.cpu() # Store on CPU: (B, S, S)

    preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()
    # Handle potential NaN from spearmanr/pearsonr if data has issues
    try:
        spearman = spearmanr(preds, trues).correlation
    except ValueError:
        logging.warning("Could not calculate Spearman correlation (possibly due to constant data).")
        spearman = np.nan
    try:
        pearson = pearsonr(preds, trues)[0]
    except ValueError:
        logging.warning("Could not calculate Pearson correlation (possibly due to constant data).")
        pearson = np.nan

    mse      = mean_squared_error(trues, preds)
    mae      = mean_absolute_error(trues, preds)
    logging.info(f"Spearman ρ={spearman:.3f}  |  Pearson r={pearson:.3f}  |  MSE={mse:.4f}  |  MAE={mae:.4f}")

    # 3) Figures
    figdir = args.fig_dir
    # -- scatter
    fig = plt.figure(figsize=(4,4))
    plt.scatter(trues, preds, alpha=0.5, s=10)
    if not np.all(np.isnan([trues.min(), preds.min(), trues.max(), preds.max()])):
      lims = [min(trues.min(), preds.min())-0.05, max(trues.max(), preds.max())+0.05]
      plt.plot(lims, lims, 'r--')
    else:
      lims = [0, 1] # Default limits if data is problematic
      plt.plot(lims, lims, 'r--', label="Ideal (y=x)")
    plt.xlabel('True'); plt.ylabel('Predicted')
    plt.title(f'Spearman ρ={spearman:.3f}')
    savefig(fig, os.path.join(figdir, 'predictions_vs_targets.png'))

    # -- distribution
    fig = plt.figure(figsize=(6,3))
    plt.hist(trues, bins=30, alpha=0.6, label='True'); plt.hist(preds, bins=30, alpha=0.6, label='Predicted')
    plt.legend(); plt.xlabel('Score'); plt.ylabel('Count'); plt.title('Score distribution')
    savefig(fig, os.path.join(figdir, 'prediction_distribution.png'))

    # -- residuals
    fig = plt.figure(figsize=(4,4))
    residuals = preds - trues
    plt.scatter(trues, residuals, alpha=0.5, s=10)
    plt.axhline(0, color='r', linestyle='--');
    plt.xlabel('True'); plt.ylabel('Residual (ŷ - y)'); plt.title('Residuals vs True')
    savefig(fig, os.path.join(figdir, 'residuals_vs_targets.png'))

    # -- attention heatmap for the FIRST image
    if first_batch_A is not None and len(first_batch_A) > 0:
        # Check if the attention map is not all zeros (which might happen if hook failed)
        if first_batch_A[0].abs().sum() == 0:
             logging.warning("First image attention map is all zeros, likely hook failed to capture weights.")
        A_first_image = first_batch_A[0].numpy() # Extract the first image's map: (S, S)
        fig = plt.figure(figsize=(6,5))
        im = plt.imshow(A_first_image, cmap='viridis') # Or 'plasma' or 'magma'
        plt.colorbar(im, fraction=0.046)
        plt.title('Self-attention for first test image')
        plt.xlabel('Key dimension'); plt.ylabel('Query dimension')
        savefig(fig, os.path.join(figdir, 'attention_heatmap_test01.png'))
    else:
        logging.warning("Could not generate attention heatmap for the first image (no data or first batch failed).")


    # -- attention pool barplot
    alpha_mean = (alpha_sum / len(preds)).cpu().numpy()
    fig = plt.figure(figsize=(8,3))
    plt.bar(np.arange(NUM_FEATURES), alpha_mean)
    plt.xlabel('Dimension index'); plt.ylabel('Mean α'); plt.title('Attention‑Pool importance')
    savefig(fig, os.path.join(figdir, 'dim_importance_barplot.png'))

    # -- raw feature correlation heatmap
    corr = np.corrcoef(X_test.T)
    fig = plt.figure(figsize=(6,5))
    im = plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)
    plt.colorbar(im, fraction=0.046)
    plt.title('Feature Pearson correlation (test)');
    savefig(fig, os.path.join(figdir, 'feature_correlation_heatmap.png'))

    # -- Integrated Gradients (slow)
    if not args.skip_ig:
        try:
            from captum.attr import IntegratedGradients
            def model_forward_for_ig(inp):
                output = model(inp, require_maps=False)
                return output

            ig = IntegratedGradients(model_forward_for_ig)
            idx = np.random.choice(len(X_test), size=min(512,len(X_test)), replace=False)
            attributions = []
            for i in tqdm(idx, desc='IG'):
                input_tensor = torch.tensor(X_test[i:i+1], dtype=torch.float32, device=device)
                attr = ig.attribute(input_tensor, target=0, internal_batch_size=64)
                attributions.append(attr.squeeze().abs().cpu().numpy())
            ig_mean = np.stack(attributions).mean(0)
            fig = plt.figure(figsize=(8,3))
            plt.bar(np.arange(NUM_FEATURES), ig_mean)
            plt.xlabel('Dimension index'); plt.ylabel('|IG|');
            plt.title('Integrated Gradients importance')
            savefig(fig, os.path.join(figdir, 'ig_importance_barplot.png'))
        except ImportError:
            logging.warning('captum not installed → skipping IG plot')
        except Exception as e:
            logging.error(f"Error during Integrated Gradients calculation: {e}", exc_info=True)


if __name__ == '__main__':
    p = argparse.ArgumentParser()
    p.add_argument('--data_dir', default='Embeddings/CLIP-HBA/IC9600', help='Folder containing static_embedding.csv & test.txt')
    p.add_argument('--weights', default='transformer_predictor_state.pth')
    p.add_argument('--scaler',  default='scaler.joblib')
    p.add_argument('--fig_dir', default='Figures')
    p.add_argument('--batch', type=int, default=128)
    p.add_argument('--cpu', action='store_true', help='Force CPU')
    p.add_argument('--skip_ig', action='store_true', help='Skip Integrated Gradients')
    args = p.parse_args()
    if not os.path.isdir(args.data_dir):
        print(f"Error: Data directory not found: {args.data_dir}")
        print("Please provide a valid path using --data_dir")
        sys.exit(1)
    main(args)
